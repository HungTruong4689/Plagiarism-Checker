{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9eaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e468c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e56fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuations(txt, punct = string.punctuation):\n",
    "    '''\n",
    "    This function will remove punctuations from the input text\n",
    "    '''\n",
    "    return ''.join([c for c in txt if c not in punct])\n",
    "  \n",
    "def remove_stopwords(txt, sw = list(stopwords.words('english'))):\n",
    "    '''\n",
    "    This function will remove the stopwords from the input txt\n",
    "    '''\n",
    "    return ' '.join([w for w in txt.split() if w.lower() not in sw])\n",
    "\n",
    "def clean_text(txt):\n",
    "    '''\n",
    "    This function will clean the text being passed by removing specific line feed characters\n",
    "    like '\\n', '\\r', and '\\'\n",
    "    '''\n",
    "    \n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('\\'', '')\n",
    "    txt = remove_punctuations(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    return txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556b3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,4),stop_words=\"english\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b71f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install spaCy library\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51197a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11202bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test lemmatization python\n",
    "string = \"Hi, World\"\n",
    "import spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "doc_spacy = en_nlp(string)\n",
    "a = [ a.lemma_ for a in doc_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e991dca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'World']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c982e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi , World'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221fe8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_file_pdf(file_path):\n",
    "    train_content = []\n",
    "    \n",
    "    pdfFileObj = open(file_path,'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    pages = pdfReader.numPages\n",
    "        \n",
    "    for i in range(pages):\n",
    "        # Creating a page object\n",
    "        pageObj = pdfReader.getPage(i)\n",
    "        text = pageObj.extractText().split(\"  \")\n",
    "        # Finally the lines are stored into list\n",
    "        # For iterating over list a loop is used\n",
    "        for i in range(len(text)):\n",
    "            train_content.append(text[i])\n",
    "    # closing the pdf file object\n",
    "    pdfFileObj.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Sentence tokenize\n",
    "    train_content = [w.replace('\\n','') for w in train_content]\n",
    "    new_train_content = [ sent_tokenize(w) for w in train_content]\n",
    "    new_content_train = []\n",
    "    for i in range(len(new_train_content)):\n",
    "        for j in range(len(new_train_content[i])):\n",
    "            new_train_content[i][j] = new_train_content[i][j].lower()\n",
    "            new_content_train.append(new_train_content[i][j])\n",
    "#     train_content_result = []\n",
    "#     en_nlp = spacy.load('en_core_web_sm')\n",
    "#     for m in range(len(new_content_train)):\n",
    "        #Test lemmatization python\n",
    "#         doc_spacy = en_nlp(new_content_train[i])\n",
    "#         a = [ a.lemma_ for a in doc_spacy]\n",
    "#         line = \" \".join(a)\n",
    "#         line = clean_text(line)\n",
    "#         line = line.lower()\n",
    "#         train_content_result.append(line)\n",
    "    return new_content_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf105653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "\n",
    "# Define the location of the directory\n",
    "path =r\"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Textual reference based figure plagiarism\\Source figures\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ed0211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [doc for doc in os.listdir() if doc.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd69b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c943faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_PATH = \"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Textual reference based figure plagiarism\\Source figures\"\n",
    "\n",
    "#train_contents = [read_files(os.path.join(OLD_PATH,File)) for File in train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d63e8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file_pdf(os.path.join(OLD_PATH,\"Source 06.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a178d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(read_file_pdf(os.path.join(OLD_PATH,\"Source 06.pdf\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa8f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = train_files.remove('source 06.pdf')\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5d9706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source 00.pdf\n",
      "437\n",
      "source 01.pdf\n",
      "14\n",
      "source 02.pdf\n",
      "12\n",
      "source 03.pdf\n",
      "808\n",
      "source 04.pdf\n",
      "283\n",
      "source 05.pdf\n",
      "160\n",
      "source 06.pdf\n",
      "source 07.pdf\n",
      "13\n",
      "source 08.pdf\n",
      "12\n",
      "source 09.pdf\n",
      "191\n",
      "source 10.pdf\n",
      "source 11.pdf\n",
      "517\n",
      "source 12.pdf\n",
      "292\n",
      "source 13.pdf\n",
      "232\n",
      "source 14.pdf\n",
      "10\n",
      "source 15.pdf\n",
      "85\n",
      "source 16.pdf\n",
      "8\n",
      "source 17.pdf\n",
      "175\n",
      "source 18.pdf\n",
      "source 19.pdf\n",
      "source 20.pdf\n",
      "8\n",
      "source 21.pdf\n",
      "10\n",
      "source 22.pdf\n",
      "source 23.pdf\n",
      "11\n",
      "source 24.pdf\n",
      "6\n",
      "source 25.pdf\n",
      "166\n",
      "source 26.pdf\n",
      "211\n",
      "source 27.pdf\n",
      "257\n",
      "source 28.pdf\n",
      "9\n",
      "source 29.pdf\n",
      "17\n",
      "source 30.pdf\n",
      "302\n",
      "source 31.pdf\n",
      "source 32.pdf\n",
      "15\n",
      "source 33.pdf\n",
      "21\n",
      "source 51.pdf\n",
      "source 52.pdf\n",
      "source 53.pdf\n",
      "0\n",
      "source 54.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "source 55.pdf\n",
      "source 56.pdf\n",
      "source 57.pdf\n",
      "218\n",
      "source 58.pdf\n",
      "23\n",
      "source 59.pdf\n",
      "13\n",
      "source 60.pdf\n",
      "source 61.pdf\n",
      "842\n",
      "source 62.pdf\n",
      "12\n",
      "source 63.pdf\n",
      "0\n",
      "source 64.pdf\n",
      "13\n",
      "source 65.pdf\n",
      "10\n",
      "source 67.pdf\n",
      "7\n",
      "source 69.pdf\n",
      "628\n",
      "source 70.pdf\n",
      "source 71.pdf\n",
      "13\n",
      "source 72.pdf\n",
      "197\n",
      "source 73.pdf\n",
      "343\n",
      "source 74.pdf\n",
      "16\n",
      "source 76.pdf\n",
      "source 77.pdf\n",
      "335\n",
      "source 78.pdf\n",
      "source 79.pdf\n",
      "15\n",
      "source 80.pdf\n",
      "229\n",
      "source 81.pdf\n",
      "240\n",
      "source 82.pdf\n",
      "19\n",
      "source 83.pdf\n",
      "source 84.pdf\n",
      "214\n",
      "source 85.pdf\n",
      "16\n",
      "source 86.pdf\n",
      "7\n",
      "source 87.pdf\n",
      "26\n",
      "source 88.pdf\n",
      "27\n",
      "source 89.pdf\n",
      "source 90.pdf\n",
      "267\n",
      "Source 92.pdf\n",
      "1525\n",
      "Source 93.pdf\n",
      "Source 94.pdf\n",
      "360\n",
      "Source 96.pdf\n",
      "219\n",
      "Source 97.pdf\n"
     ]
    }
   ],
   "source": [
    "for i in train_files:\n",
    "    try:\n",
    "        print(i)\n",
    "        print(len(read_file_pdf(os.path.join(OLD_PATH,i)) ))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "746401af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the directory\n",
    "path =r\"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Textual reference based figure plagiarism\\Source figures - Copy\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35c3e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [doc for doc in os.listdir() if doc.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4759335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3975007",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_PATH = \"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Textual reference based figure plagiarism\\Source figures - Copy\"\n",
    "\n",
    "train_contents = [read_file_pdf(os.path.join(OLD_PATH,File)) for File in train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0db29a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'web object retrieval zaiqing nie, yunxiao ma, shuming shi, ji-rong wen, wei-ying ma microsoft research asia, beijing, china {znie, yunxiaom, shumings, jrwen, wyma}@microsoft.com  abstract the primary function of current web search engines is essentially relevance ranking at the document level.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96c4557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents_clean = train_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "252d5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents_clean= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58f327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents_clean= []\n",
    "train_content_sub = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1814211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "for i in range(len(train_contents)):\n",
    "    for j in range(len(train_contents[i])):\n",
    "        doc_spacy = en_nlp(train_contents[i][j])\n",
    "        line = [ a.lemma_ for a in doc_spacy]\n",
    "        sentence = \" \".join(line)\n",
    "        sentence = remove_punctuations(sentence)\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        train_content_sub.append(sentence)\n",
    "    train_contents_clean.append(train_content_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d6612aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_contents_clean[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4420c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'web object retrieval zaiqing nie , yunxiao ma , shume shi , ji - rong wen , wei - ying ma microsoft research asia , beijing , china { znie , yunxiaom , shuming , jrwen , wyma}@microsoft.com   abstract the primary function of current web search engine be essentially relevance rank at the document level .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(train_contents_clean[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c28499a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents_clean_sentence= []\n",
    "train_content_sub_2 = []\n",
    "for i in range(len(train_contents_clean)):\n",
    "    for j in range(len(train_contents_clean[i])):\n",
    "        sentence = \" \".join(train_contents_clean[i][j])\n",
    "        sentence = remove_punctuations(sentence)\n",
    "        sentence = remove_stopwords(sentence)\n",
    "        train_content_sub_2.append(sentence)\n",
    "    train_contents_clean_sentence.append(train_content_sub_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2138b40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'web object retrieval zaiqing nie yunxiao shume shi ji rong wen wei ying microsoft research asia beijing china znie yunxiaom shuming jrwen wymamicrosoftcom abstract primary function current web search engine essentially relevance rank document level'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_contents_clean_sentence[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents_clean_sentence[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3cdbbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_train = [line for line in train_contents]\n",
    "s_vectors = list(zip(train_files,train_contents_clean_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f2beaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'source 00.pdf'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_vectors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "684ae53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,4),stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4473519",
   "metadata": {},
   "source": [
    "## Test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d36fcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = ['The contents of a Web object are aggregated from multiple Web sources. These copies may be  inconsistent  because  of  the  diverse  Web site  qualities  and  the  limited  performance  of current information extraction techniques.',\n",
    "            'From  each  source,  two  steps  are  taken  to extract  the  wanted  information.  First,  record extraction  [6]  is  applied  to  get  data  records relevant  to  the  domain  from  the  resource.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de3aa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_clean = []\n",
    "for i in range(len(test_file)):\n",
    "    test_file[i] = clean_text(test_file[i])\n",
    "    test_file_clean.append(test_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e6b56fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contents web object aggregated multiple web sources copies may inconsistent diverse web site qualities limited performance current information extraction techniques',\n",
       " 'source two steps taken extract wanted information first record extraction 6 applied get data records relevant domain resource']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8effbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_clean = []\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "for i in range(len(test_file)):\n",
    "    doc_spacy = en_nlp(test_file[i])\n",
    "    line = [ a.lemma_ for a in doc_spacy]\n",
    "    sentence = \" \".join(line)\n",
    "    test_file_clean.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0b3fff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content web object aggregate multiple web source copy may inconsistent diverse web site quality limited performance current information extraction technique',\n",
       " 'source two step take extract want information first record extraction 6 apply get data record relevant domain resource']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbac2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plagiarism(s_vectors,test_file):\n",
    "    test_result= {}\n",
    "    for test_line in range(len(test_file)):\n",
    "        test_result[\"line\"+str(test_line)] =dict()\n",
    "        for a, vector_a in s_vectors:\n",
    "            test_result[\"line\"+str(test_line)][a] = dict()\n",
    "            for train_line in range(len(vector_a)):\n",
    "                documents = [vector_a[train_line],test_file[test_line]]\n",
    "                sparse_matrix = vectorizer.fit_transform(documents)\n",
    "                score = cosine_similarity(sparse_matrix)[0][1]\n",
    "                test_result[\"line\"+str(test_line)][a][\"line\"+str(train_line)] = score\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f3b58f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_plagiarism' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0dd54be267d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_plagiarism\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_vectors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'check_plagiarism' is not defined"
     ]
    }
   ],
   "source": [
    "test_result = check_plagiarism(s_vectors,test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt = pd.DataFrame(test_result[\"line0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_for_max_values_of(df):\n",
    "    A = df.columns[df.isin([max(df.T.max())]).any()].item() \n",
    "    B = df.T.columns[df.T.isin([max(df.max())]).any()].item()\n",
    "    return A, B, max(df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40724b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
