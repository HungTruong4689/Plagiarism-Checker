{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d287899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b891f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "\n",
    "# Define the location of the directory\n",
    "path =r\"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Shape_Based_Figure Plagiarism\\Source figures\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuations(txt, punct = string.punctuation):\n",
    "    '''\n",
    "    This function will remove punctuations from the input text\n",
    "    '''\n",
    "    return ''.join([c for c in txt if c not in punct])\n",
    "  \n",
    "def remove_stopwords(txt, sw = list(stopwords.words('english'))):\n",
    "    '''\n",
    "    This function will remove the stopwords from the input txt\n",
    "    '''\n",
    "    return ' '.join([w for w in txt.split() if w.lower() not in sw])\n",
    "\n",
    "def clean_text(txt):\n",
    "    '''\n",
    "    This function will clean the text being passed by removing specific line feed characters\n",
    "    like '\\n', '\\r', and '\\'\n",
    "    '''\n",
    "    \n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('\\'', '')\n",
    "    txt = remove_punctuations(txt)\n",
    "    txt = remove_stopwords(txt)\n",
    "    return txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf415700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file_path):\n",
    "    result = []\n",
    "    with open(file_path, encoding=\"cp437\") as file:\n",
    "        for line in file.readlines():\n",
    "            a = line.find(\"coordinate\")\n",
    "            line = line[:a]\n",
    "            line = clean_text(line)\n",
    "            line = line.replace(\"type\",\"\")\n",
    "            line = line.replace(\"text\",\"\")\n",
    "            line = line.lower()\n",
    "            result.append(line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7095db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa47f07",
   "metadata": {},
   "source": [
    "## Build training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [doc for doc in os.listdir() if doc.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f17e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd743b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_PATH = \"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Shape_Based_Figure Plagiarism\\Source figures/\"\n",
    "\n",
    "train_contents = [read_files(os.path.join(OLD_PATH,File)) for File in train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_files(os.path.join(OLD_PATH,\"Source-figure-100_out.txt\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eaeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,4),stop_words=\"english\")\n",
    "#vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity = lambda doc1,doc2: cosine_similarity([doc1,doc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99856a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_train = [line for line in train_contents]\n",
    "s_vectors = list(zip(train_files,vector_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vectors[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the directory\n",
    "path2 =r\"D:\\data\\Work project\\Check Plagiarism\\checkdata\\testfile\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdff4bb",
   "metadata": {},
   "source": [
    "## Check test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the test file\n",
    "test_name = \"suspicious-figure-0_out.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLD_PATH = \"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Shape_Based_Figure Plagiarism\\Plagiarised figures\\Exact copy\"\n",
    "\n",
    "test_file = read_files(os.path.join(OLD_PATH,test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949516f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110acf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15205f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result(test_file):\n",
    "    result = dict()\n",
    "    for i in range(len(test_file)):\n",
    "        result[\"line\"+str(i)] = dict()\n",
    "        result[\"line\"+str(i)][\"name\"] = []\n",
    "        result[\"line\"+str(i)][\"point\"] = []\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plagiarism(s_vectors,test_file):\n",
    "    test_result= {}\n",
    "    for test_line in range(len(test_file)):\n",
    "        test_result[\"line\"+str(test_line)] =dict()\n",
    "        for a, vector_a in s_vectors:\n",
    "            test_result[\"line\"+str(test_line)][a] = dict()\n",
    "            for train_line in range(len(vector_a)):\n",
    "                documents = [vector_a[train_line],test_file[test_line]]\n",
    "                sparse_matrix = vectorizer.fit_transform(documents)\n",
    "                score = cosine_similarity(sparse_matrix)[0][1]\n",
    "                test_result[\"line\"+str(test_line)][a][\"line\"+str(train_line)] = score\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cd655",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = check_plagiarism(s_vectors,test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_line in range(len(test_file)):\n",
    "    test_result[\"line\"+str(test_line)] =dict()\n",
    "    for a, vector_a in s_vectors:\n",
    "        #print(a)\n",
    "        test_result[\"line\"+str(test_line)][a] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad0881",
   "metadata": {},
   "source": [
    "## Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040545b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt = pd.DataFrame(test_result[\"line0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb55c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_for_max_values_of(df):\n",
    "    A = df.columns[df.isin([max(df.T.max())]).any()].item() \n",
    "    B = df.T.columns[df.T.isin([max(df.max())]).any()].item()\n",
    "    return A, B, max(df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_name_for_max_values_of(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7d797",
   "metadata": {},
   "source": [
    "## Check plagiarism with not exact data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_PATH2 = \"D:\\data\\Work project\\Check Plagiarism\\Dataset\\Figure Plagiarism corpus\\Shape_Based_Figure Plagiarism\\Plagiarised figures\\Text plagiarism\"\n",
    "test_name2 = \"Suspicious-figure-275_out.txt\"\n",
    "test_file2 = read_files(os.path.join(OLD_PATH2,test_name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = check_plagiarism(s_vectors,test_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8745d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = pd.DataFrame(result2[\"line0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_name_for_max_values_of(dt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e7f98",
   "metadata": {},
   "source": [
    "## Second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd469fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# from gensim import corpora\n",
    "# import gensim.downloader as api\n",
    "# from gensim.utils import simple_preprocess\n",
    "# print(gensim.__version__)\n",
    "\n",
    "\n",
    "# # Download the FastText model\n",
    "# fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80687a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plagiarism_soft(s_vectors,test_file):\n",
    "    test_result= {}\n",
    "    for test_line in range(len(test_file)):\n",
    "        test_result[\"line\"+str(test_line)] =dict()\n",
    "        for a, vector_a in s_vectors:\n",
    "            test_result[\"line\"+str(test_line)][a] = dict()\n",
    "            for train_line in range(len(vector_a)):\n",
    "#                 lemmatized_trainline = [wordnet_lemmatizer.lemmatize(t) for t in vector_a[train_line]]\n",
    "#                 lemmatized_testline = [wordnet_lemmatizer.lemmatize(t) for t in test_file[test_line]]\n",
    "                documents = [vector_a[train_line],test_file[test_line]]\n",
    "                dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "                similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "                #sparse_matrix = vectorizer.fit_transform(documents)\n",
    "                sent_1 = dictionary.doc2bow(simple_preprocess(vector_a[train_line]))\n",
    "                sent_2 = dictionary.doc2bow(simple_preprocess(test_file[test_line]))\n",
    "                sentences = [sent_1,sent_2]\n",
    "                score = cosine_similarity(sentences)[0][1]\n",
    "                test_result[\"line\"+str(test_line)][a][\"line\"+str(train_line)] = score\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82975d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \" test this line with changing some line\"\n",
    "alpha_only = word_tokenize(a)\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in alpha_only]\n",
    "lemmatized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33303e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24eb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result3 = check_plagiarism_soft(s_vectors,test_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ecf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ff6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
